{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading US dataset\n",
      "finished loading US topic variables...\n",
      "loading us corpus file\n",
      "finish loading corpus variables...\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from  lda_gibbs import GibbsLDA\n",
    "from corpus import corpus, document\n",
    "\n",
    "print \"loading US dataset\"\n",
    "us_01_topic_variables_pickle_file = '/Users/xiwang/sigir_topis_90/sigir.p_gibbsLDA__K_90_alpha_0.4_1516989047418vis.p'\n",
    "us_01_topic_variables = pickle.load(open(us_01_topic_variables_pickle_file,'rb'))\n",
    "us_02_topic_variables_pickle_file = '/Users/xiwang/sigir_topis_90/sigir.p_gibbsLDA__K_90_alpha_0.4_1516989061703vis.p'\n",
    "us_02_topic_variables = pickle.load(open(us_02_topic_variables_pickle_file,'rb'))\n",
    "us_03_topic_variables_pickle_file = '/Users/xiwang/sigir_topis_90/sigir.p_gibbsLDA__K_90_alpha_0.4_1516989098256vis.p'\n",
    "us_03_topic_variables = pickle.load(open(us_03_topic_variables_pickle_file,'rb'))\n",
    "us_04_topic_variables_pickle_file = '/Users/xiwang/sigir_topis_90/sigir.p_gibbsLDA__K_90_alpha_0.4_1516989290114vis.p'\n",
    "us_04_topic_variables = pickle.load(open(us_04_topic_variables_pickle_file,'rb'))\n",
    "us_05_topic_variables_pickle_file = '/Users/xiwang/sigir_topis_90/sigir.p_gibbsLDA__K_90_alpha_0.4_1516989297742vis.p'\n",
    "us_05_topic_variables = pickle.load(open(us_05_topic_variables_pickle_file,'rb'))\n",
    "us_06_topic_variables_pickle_file = '/Users/xiwang/sigir_topis_90/sigir.p_gibbsLDA__K_90_alpha_0.4_1516989298108vis.p'\n",
    "us_06_topic_variables = pickle.load(open(us_06_topic_variables_pickle_file,'rb'))\n",
    "us_07_topic_variables_pickle_file = '/Users/xiwang/sigir_topis_90/sigir.p_gibbsLDA__K_90_alpha_0.4_1516989299347vis.p'\n",
    "us_07_topic_variables = pickle.load(open(us_07_topic_variables_pickle_file,'rb'))\n",
    "us_08_topic_variables_pickle_file = '/Users/xiwang/sigir_topis_90/sigir.p_gibbsLDA__K_90_alpha_0.4_1516989317230vis.p'\n",
    "us_08_topic_variables = pickle.load(open(us_08_topic_variables_pickle_file,'rb'))\n",
    "us_09_topic_variables_pickle_file = '/Users/xiwang/sigir_topis_90/sigir.p_gibbsLDA__K_90_alpha_0.4_1516989357358vis.p'\n",
    "us_09_topic_variables = pickle.load(open(us_09_topic_variables_pickle_file,'rb'))\n",
    "us_10_topic_variables_pickle_file = '/Users/xiwang/sigir_topis_90/sigir.p_gibbsLDA__K_90_alpha_0.4_1516989474150vis.p'\n",
    "us_10_topic_variables = pickle.load(open(us_10_topic_variables_pickle_file,'rb'))\n",
    "print \"finished loading US topic variables...\"\n",
    "\n",
    "print \"loading us corpus file\"\n",
    "us_corpus_pickle_file = '/Users/xiwang/sigir_topis_90/sigir.p'\n",
    "us_f = open(us_corpus_pickle_file,'rb')\n",
    "us_corpus = pickle.load(us_f)\n",
    "print \"finish loading corpus variables...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import operator\n",
    "from sklearn import preprocessing\n",
    "from gensim.matutils import hellinger\n",
    "import requests\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cluster import KMeans\n",
    "'''\n",
    "    Preparing the comarison data between JS_divergence and Hellinger_distance\n",
    "'''\n",
    "from scipy.stats import entropy\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "'''\n",
    "    Calculate the JS Divergence\n",
    "'''\n",
    "def JSD(P, Q):\n",
    "    _P = P / norm(P, ord=1)\n",
    "    _Q = Q / norm(Q, ord=1)\n",
    "    _M = 0.5 * (_P + _Q)\n",
    "    return 0.5 * (entropy(_P, _M) + entropy(_Q, _M))\n",
    "\n",
    "# uk\n",
    "uk_topic_URL = \"http://130.209.249.33:12345/topic\"\n",
    "uk_word_URL = \"http://130.209.249.33:12345/words\"\n",
    "\n",
    "# us\n",
    "us_topic_URL = \"http://130.209.249.31:12346/topic\"\n",
    "us_word_URL = \"http://130.209.249.31:12346/words\"\n",
    "\n",
    "# aggregate topic_variables\n",
    "#uk_topic_variable = [uk_01_topic_variables, uk_02_topic_variables, uk_03_topic_variables, uk_04_topic_variables, uk_05_topic_variables]\n",
    "us_topic_variable = [us_01_topic_variables, us_02_topic_variables, us_03_topic_variables, us_04_topic_variables, us_05_topic_variables, \n",
    "                     us_06_topic_variables, us_07_topic_variables, us_08_topic_variables, us_09_topic_variables, us_10_topic_variables]\n",
    "\n",
    "'''\n",
    "    Initialize variables from pickle file, and create corresponding key variable list for each topic variable\n",
    "'''\n",
    "vocab = []\n",
    "doc_lengths = []\n",
    "topic_term_dists = []\n",
    "term_frequency = []\n",
    "doc_topic_dists = []\n",
    "\n",
    "for var in us_topic_variable:\n",
    "    vocab.append(var['vocab'])\n",
    "    doc_lengths.append(var['doc_lengths'])\n",
    "    topic_term_dists.append(var['topic_term_dists'])\n",
    "    term_frequency.append(var['term_frequency'])\n",
    "    doc_topic_dists.append(var['doc_topic_dists'])\n",
    "# aggregate the topic txt file\n",
    "#uk_topic_txt_file = [\"/Volumes/xiwang/topicsim/uk.p_gibbsLDA__K_1000_alpha_0.4_1510580677822.topic.txt\",\n",
    "#                     \"/Volumes/xiwang/topicsim/uk.p_gibbsLDA__K_1000_alpha_0.4_1510580766980.topic.txt\",\n",
    "#                     \"/Volumes/xiwang/topicsim/uk.p_gibbsLDA__K_1000_alpha_0.4_1510581056713.topic.txt\",\n",
    "#                     \"/Volumes/xiwang/topicsim/uk.p_gibbsLDA__K_1000_alpha_0.4_1510582766580.topic.txt\",\n",
    "#                     \"/Volumes/xiwang/topicsim/uk.p_gibbsLDA__K_1000_alpha_0.4_1510585895835.topic.txt\"]\n",
    "\n",
    "us_topic_txt_file = [\"/Users/xiwang/sigir_topis_90/sigir.p_gibbsLDA__K_90_alpha_0.4_1516989047418.topic.txt\",\n",
    "                     \"/Users/xiwang/sigir_topis_90/sigir.p_gibbsLDA__K_90_alpha_0.4_1516989061703.topic.txt\",\n",
    "                     \"/Users/xiwang/sigir_topis_90/sigir.p_gibbsLDA__K_90_alpha_0.4_1516989098256.topic.txt\",\n",
    "                     \"/Users/xiwang/sigir_topis_90/sigir.p_gibbsLDA__K_90_alpha_0.4_1516989290114.topic.txt\",\n",
    "                     \"/Users/xiwang/sigir_topis_90/sigir.p_gibbsLDA__K_90_alpha_0.4_1516989297742.topic.txt\",\n",
    "                     \"/Users/xiwang/sigir_topis_90/sigir.p_gibbsLDA__K_90_alpha_0.4_1516989298108.topic.txt\",\n",
    "                     \"/Users/xiwang/sigir_topis_90/sigir.p_gibbsLDA__K_90_alpha_0.4_1516989299347.topic.txt\",\n",
    "                     \"/Users/xiwang/sigir_topis_90/sigir.p_gibbsLDA__K_90_alpha_0.4_1516989317230.topic.txt\",\n",
    "                     \"/Users/xiwang/sigir_topis_90/sigir.p_gibbsLDA__K_90_alpha_0.4_1516989357358.topic.txt\",\n",
    "                     \"/Users/xiwang/sigir_topis_90/sigir.p_gibbsLDA__K_90_alpha_0.4_1516989474150.topic.txt\",\n",
    "                     ]\n",
    "\n",
    "us_start_time = 1470006000000\n",
    "us_end_time = 1477958399000\n",
    "\n",
    "uk_start_time = 1504249200000\n",
    "uk_end_time = 1506841199000\n",
    "\n",
    "def getWordSimilarity(w1, w2, word_URL):\n",
    "    words = {'w1':w1, 'w2':w2}\n",
    "    resp = requests.post(word_URL,json=words)\n",
    "    return float(resp.json()['Similairty'])\n",
    "\n",
    "def getTwoTopicsDistance_vecsum(topic1, topic2, word_URL):\n",
    "    words1 = topic1.split(',')\n",
    "    words2 = topic2.split(',')\n",
    "    sumV = 0.0 \n",
    "    count = 0.0\n",
    "    for w1 in words1:\n",
    "        for w2 in words2:\n",
    "            sumV += getWordSimilarity(w1, w2, word_URL)\n",
    "            count += 1\n",
    "    sumV /= count\n",
    "    return sumV \n",
    "\n",
    "def getTwoTopicsDistance_minipath(topic1, topic2, word_URL):\n",
    "    words1 = topic1.split(',')\n",
    "    words2 = topic2.split(',')\n",
    "    sumV = 0.0 \n",
    "    for w1 in words1:\n",
    "        if w1[0] == \"#\" or w1[0] == \"@\":\n",
    "            w1 = w1[1:]\n",
    "    for w2 in words2:\n",
    "        if w2[0] == \"#\" or w2[0] == \"@\":\n",
    "            w2 = w2[1:]\n",
    "    for w1 in words1:\n",
    "        tmp = [getWordSimilarity(w1, w2,word_URL) for w2 in words2]\n",
    "        sumV += sorted(tmp, reverse=True)[0]\n",
    "    return sumV\n",
    "\n",
    "def getTwoTopicsDistance_thred(topic1, topic2, word_URL, thred=0.1):\n",
    "    words1 = topic1.split(',')\n",
    "    words2 = topic2.split(',')\n",
    "    count = 0.0 \n",
    "    for w1 in words1:\n",
    "        tmp = [getWordSimilarity(w1, w2, word_URL) for w2 in words2]\n",
    "        count += sum([1 for t in tmp if t >= thred])\n",
    "    return count\n",
    "\n",
    "def topic_coh(topic_s,topic_URL):\n",
    "    topic ={'topic':topic_s}\n",
    "    resp = requests.post(topic_URL, json=topic)\n",
    "    return float(resp.json()['Similairty'])\n",
    "\n",
    "def cosine_similarity_new(topic_dist_1, topic_dist_2):\n",
    "    # get the index of the 15 highest value in two topic term distribution\n",
    "    max_index_1 = np.argsort(topic_dist_1)[-50:][::-1]\n",
    "    max_index_2 = np.argsort(topic_dist_2)[-50:][::-1]\n",
    "    max_index = np.unique(np.concatenate((max_index_1,max_index_2),0))\n",
    "    #print max_index\n",
    "    selected_dist_1 = [topic_dist_1[x] for x in max_index]\n",
    "    selected_dist_2 = [topic_dist_2[x] for x in max_index]\n",
    "    return cosine_similarity([selected_dist_1],[selected_dist_2])[0][0]\n",
    "\n",
    "    resp = requests.post(topic_URL, json=topic)\n",
    "    return float(resp.json()['Similairty'])\n",
    "\n",
    "def cosine_similarity_new(topic_dist_1, topic_dist_2):\n",
    "    # get the index of the 15 highest value in two topic term distribution\n",
    "    max_index_1 = np.argsort(topic_dist_1)[-50:][::-1]\n",
    "    max_index_2 = np.argsort(topic_dist_2)[-50:][::-1]\n",
    "    max_index = np.unique(np.concatenate((max_index_1,max_index_2),0))\n",
    "    #print max_index\n",
    "    selected_dist_1 = [topic_dist_1[x] for x in max_index]\n",
    "    selected_dist_2 = [topic_dist_2[x] for x in max_index]\n",
    "    return cosine_similarity([selected_dist_1],[selected_dist_2])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print len(topic_term_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Generate 15 terms for each topic with highest frequency\n",
    "'''\n",
    "topic_top_term_list = []\n",
    "for k in range(len(us_topic_variable)):\n",
    "    vocab = us_topic_variable[k]['vocab']\n",
    "    #topic_term_dists = us_topic_variable[k]['topic_term_dists']\n",
    "    topic_top_term_sub_list = []\n",
    "    for i in range(len(topic_term_dists[k])):\n",
    "        vocab_frequency_dict={vocab[j] : topic_term_dists[k][i][j] for j in range(len(vocab))}\n",
    "        sorted_term = sorted(vocab_frequency_dict, key=vocab_frequency_dict.get, reverse=True)\n",
    "        topic_top_term_sub_list.append(','.join(sorted_term[:15]))\n",
    "    topic_top_term_list.append(topic_top_term_sub_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print len(topic_term_dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare user study data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['048', '029', '068', '083', '005', '022', '010', '057', '038', '075', '051', '032', '036', '003', '061', '008', '011', '077', '055', '020', '033', '050', '082', '085', '034', '066', '062', '089', '087', '019', '120', '160', '105', '129', '138', '187', '164', '180', '101', '183', '158', '165', '161', '123', '121', '175', '130', '113', '142', '134', '102', '149', '173', '137', '189', '115', '122', '139', '128', '179', '271', '238', '211', '252', '214', '277', '287', '274', '232', '233', '222', '272', '255', '230', '267', '251', '244', '265', '219', '240', '278', '234', '203', '280', '216', '279', '249', '253', '217', '282', '304', '387', '340', '344', '375', '374', '335', '339', '352', '314', '303', '372', '360', '320', '379', '322', '384', '327', '371', '309', '354', '324', '300', '333', '306', '338', '361', '346', '316', '362', '409', '476', '481', '478', '469', '473', '418', '447', '402', '455', '448', '482', '444', '471', '419', '429', '413', '474', '420', '430', '470', '442', '421', '461', '467', '462', '437', '435', '401', '415', '565', '536', '530', '567', '504', '559', '511', '571', '588', '579', '558', '576', '562', '578', '517', '528', '508', '560', '577', '505', '535', '586', '543', '561', '512', '541', '580', '525', '568', '546', '646', '627', '680', '658', '603', '647', '622', '609', '632', '681', '616', '610', '605', '653', '688', '635', '608', '651', '657', '677', '689', '621', '659', '674', '624', '626', '656', '670', '643', '601', '717', '761', '701', '736', '748', '751', '709', '783', '746', '710', '780', '757', '730', '732', '775', '706', '753', '749', '776', '756', '785', '764', '708', '742', '725', '787', '729', '740', '728', '734', '818', '855', '827', '886', '869', '804', '831', '872', '848', '837', '825', '856', '817', '852', '845', '814', '889', '823', '875', '830', '866', '876', '812', '805', '839', '888', '879', '884', '808', '834', '913', '945', '957', '920', '977', '908', '971', '958', '935', '903', '969', '978', '966', '922', '910', '964', '939', '944', '941', '981', '988', '923', '900', '946', '921', '962', '940', '959', '983', '980']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Preparing the base topic set\n",
    "'''\n",
    "base_topic_set = []\n",
    "for idx, var in enumerate(us_topic_variable):\n",
    "    #print idx\n",
    "    '''\n",
    "        Initialize the words of each topic\n",
    "    '''\n",
    "    topic_txt_dict = {}\n",
    "    with open(us_topic_txt_file[idx]) as text_f:\n",
    "        lines = text_f.readlines()\n",
    "        for line in lines:\n",
    "            values = line.split('\\t')\n",
    "            topic_txt_dict[values[0]] = values[1]\n",
    "\n",
    "    # rank the topic and find the top 30 coherence topics from topic list\n",
    "    coh = {k:topic_coh(topic_txt_dict[k], us_topic_URL) for k in topic_txt_dict}\n",
    "    base_topic = sorted(coh, key=coh.get, reverse=True)[:30]\n",
    "    for i in range(len(base_topic)):\n",
    "        base_topic[i] = int(base_topic[i])\n",
    "        base_topic_set.append(str(idx) + '{0:02d}'.format(base_topic[i]))\n",
    "print base_topic_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "we_sim_score = pickle.load(open(\"word_embedding_score_update.p\",\"rb\"))\n",
    "for k in range(len(us_topic_variable)):\n",
    "    for i in range(len(topic_top_term_list[k])):\n",
    "        for j in range(i):\n",
    "            we_sim_score[k,i,j] = we_sim_score[k,j,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sig360HD58JS27\n",
      "\n",
      "1 sig134HD39JS60\n",
      "\n",
      "2 sig578JS36HD29\n",
      "\n",
      "3 sig121JS58HD42\n",
      "\n",
      "4 sig839JS02HD77\n",
      "\n",
      "5 sig823JS09HD53\n",
      "\n",
      "6 sig852HD10JS04\n",
      "\n",
      "7 sig708JS13HD52\n",
      "\n",
      "8 sig528HD30JS71\n",
      "\n",
      "9 sig137HD52JS02\n",
      "\n",
      "10 sig102HD23JS52\n",
      "\n",
      "11 sig020JS37HD23\n",
      "\n",
      "12 sig706JS49HD35\n",
      "\n",
      "13 sig437JS21HD48\n",
      "\n",
      "14 sig120HD04JS60\n",
      "\n",
      "15 sig734HD54JS03\n",
      "\n",
      "16 sig189HD28JS86\n",
      "\n",
      "17 sig010HD76JS48\n",
      "\n",
      "18 sig944HD48JS52\n",
      "\n",
      "19 sig055HD02JS32\n",
      "\n",
      "20 sig421JS47HD37\n",
      "\n",
      "21 sig869HD25JS27\n",
      "\n",
      "22 sig814JS10HD85\n",
      "\n",
      "23 sig361HD02JS84\n",
      "\n",
      "24 sig504HD09JS60\n",
      "\n",
      "25 sig530HD28JS36\n",
      "\n",
      "26 sig061JS57HD69\n",
      "\n",
      "27 sig173HD74JS59\n",
      "\n",
      "28 sig756HD35JS06\n",
      "\n",
      "29 sig130HD33JS22\n",
      "\n",
      "30 sig077HD17JS22\n",
      "\n",
      "31 sig757HD52JS44\n",
      "\n",
      "32 sig958JS06HD68\n",
      "\n",
      "33 sig626HD07JS35\n",
      "\n",
      "34 sig621HD72JS01\n",
      "\n",
      "35 sig505JS36HD43\n",
      "\n",
      "36 sig304HD36JS00\n",
      "\n",
      "37 sig804HD21JS52\n",
      "\n",
      "38 sig320HD12JS45\n",
      "\n",
      "39 sig279JS71HD58\n",
      "\n",
      "40 sig482JS78HD63\n",
      "\n",
      "41 sig335JS40HD78\n",
      "\n",
      "42 sig560JS04HD62\n",
      "\n",
      "43 sig749JS10HD06\n",
      "\n",
      "44 sig324HD53JS54\n",
      "\n",
      "45 sig674JS81HD05\n",
      "\n",
      "46 sig300HD63JS11\n",
      "\n",
      "47 sig033HD51JS11\n",
      "\n",
      "48 sig845HD40JS79\n",
      "\n",
      "49 sig219JS75HD04\n",
      "\n",
      "380 120\n",
      "Finish Preparing JS HD topic pair\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with open(\"sigirtopicset.txt\",\"a\") as f:\n",
    "    '''\n",
    "        Select 50 topic from 300 base topic \n",
    "        could generate different candidiate topic based on selected approach\n",
    "    '''\n",
    "    Count_Topic = 0\n",
    "    candidate_set = set()\n",
    "    candidate_set.clear()\n",
    "    #base_topic_set_copy = base_topic_set[:]\n",
    "    #while Count_Topic < 50:\n",
    "    count_equal = 0\n",
    "    count_unequal = 0\n",
    "    base_topic_set_copy = base_topic_set[:]\n",
    "    while Count_Topic < 50:\n",
    "    #for base_topic_index in range(len(base_topic_set)):\n",
    "        base_topic_index = random.randint(0,len(base_topic_set_copy)-1)\n",
    "        #base_topic_index = i\n",
    "        base_topic = base_topic_set_copy[base_topic_index]\n",
    "        indx = int(base_topic[0])\n",
    "        base_topic_indx = int(base_topic[1:3])\n",
    "        #topic_term_dists = np.array(us_topic_variable[indx]['topic_term_dists'])\n",
    "        JS_sim = {k : (1- JSD(topic_term_dists[indx][base_topic_indx], topic_term_dists[indx][k])) for k in range(len(topic_term_dists[indx])) if k != base_topic_indx}\n",
    "        JS_topic = max(JS_sim.iteritems(), key=operator.itemgetter(1))[0]\n",
    "        JS_choice = sorted(JS_sim, key=JS_sim.get, reverse=True)[:10]\n",
    "        HD_sim = {k : (1- hellinger(topic_term_dists[indx][base_topic_indx], topic_term_dists[indx][k])) for k in range(len(topic_term_dists[indx])) if k != base_topic_indx} \n",
    "        HD_topic = max(HD_sim.iteritems(), key=operator.itemgetter(1))[0]\n",
    "        HD_choice = sorted(HD_sim, key=HD_sim.get, reverse=True)[:10]\n",
    "        #print JS_topic,HD_topic\n",
    "        selected_topic = '{0:02d}'.format(JS_topic) + '{0:02d}'.format(HD_topic)\n",
    "        if JS_topic != HD_topic and selected_topic not in candidate_set:\n",
    "            candidate_set.add(selected_topic)\n",
    "            del base_topic_set_copy[base_topic_index]\n",
    "            candidate_list = [\"JS\" + '{0:02d}'.format(JS_topic),\"HD\" + '{0:02d}'.format(HD_topic)]\n",
    "            shuffle(candidate_list)\n",
    "            topic_set = \"sig\" + base_topic + candidate_list[0] + candidate_list[1] + \"\\n\"\n",
    "            print Count_Topic, topic_set\n",
    "            Count_Topic += 1 \n",
    "            f.write(topic_set)\n",
    "            #count_unequal += 1\n",
    "            for i in range(len(HD_choice)):\n",
    "                if HD_choice[i] in JS_choice:\n",
    "                    count_equal += 1\n",
    "                else:\n",
    "                    count_unequal += 1\n",
    "        else:\n",
    "            #count_equal += 1\n",
    "            del base_topic_set_copy[base_topic_index]\n",
    "    print count_equal, count_unequal\n",
    "f.close() \n",
    "print \"Finish Preparing JS HD topic pair\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sig267JS89CS70\n",
      "\n",
      "1 sig361CS10JS84\n",
      "\n",
      "2 sig601CS21JS32\n",
      "\n",
      "3 sig869CS63JS27\n",
      "\n",
      "4 sig677JS61CS48\n",
      "\n",
      "5 sig962CS88JS22\n",
      "\n",
      "6 sig437JS21CS48\n",
      "\n",
      "7 sig748JS30CS04\n",
      "\n",
      "8 sig138JS87CS71\n",
      "\n",
      "9 sig338JS83CS05\n",
      "\n",
      "10 sig706CS35JS49\n",
      "\n",
      "11 sig817CS44JS85\n",
      "\n",
      "12 sig512JS70CS75\n",
      "\n",
      "13 sig461CS49JS72\n",
      "\n",
      "14 sig139CS34JS51\n",
      "\n",
      "15 sig113JS60CS47\n",
      "\n",
      "16 sig834CS77JS02\n",
      "\n",
      "17 sig008JS60CS01\n",
      "\n",
      "18 sig435CS10JS67\n",
      "\n",
      "19 sig643JS35CS17\n",
      "\n",
      "20 sig944JS52CS48\n",
      "\n",
      "21 sig419CS80JS88\n",
      "\n",
      "22 sig504JS60CS09\n",
      "\n",
      "23 sig010CS76JS48\n",
      "\n",
      "24 sig455CS39JS22\n",
      "\n",
      "25 sig134JS60CS39\n",
      "\n",
      "26 sig068JS83CS72\n",
      "\n",
      "27 sig812CS58JS40\n",
      "\n",
      "28 sig977CS78JS03\n",
      "\n",
      "29 sig775CS00JS12\n",
      "\n",
      "30 sig120JS60CS04\n",
      "\n",
      "31 sig478JS82CS83\n",
      "\n",
      "32 sig033CS51JS11\n",
      "\n",
      "33 sig123JS52CS02\n",
      "\n",
      "34 sig320JS45CS12\n",
      "\n",
      "35 sig517CS27JS43\n",
      "\n",
      "36 sig300CS63JS11\n",
      "\n",
      "37 sig940JS29CS34\n",
      "\n",
      "38 sig062CS69JS47\n",
      "\n",
      "39 sig656JS51CS78\n",
      "\n",
      "40 sig535CS60JS48\n",
      "\n",
      "41 sig102JS52CS23\n",
      "\n",
      "42 sig244JS16CS48\n",
      "\n",
      "43 sig354CS67JS16\n",
      "\n",
      "44 sig130JS22CS33\n",
      "\n",
      "45 sig866CS62JS57\n",
      "\n",
      "46 sig142JS56CS17\n",
      "\n",
      "47 sig823JS09CS53\n",
      "\n",
      "48 sig173JS59CS31\n",
      "\n",
      "49 sig530JS36CS54\n",
      "\n",
      "Finish Preparing JS CS topic pair\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Preparing the comarison data between JS_divergence and Cosine Similarity\n",
    "'''\n",
    "\n",
    "with open(\"sigirtopicset.txt\",\"a\") as f:\n",
    "    '''\n",
    "        Select 50 topic from 300 base topic \n",
    "        could generate different candidiate topic based on selected approach\n",
    "    '''\n",
    "    Count_Topic = 0\n",
    "    candidate_set = set()\n",
    "    candidate_set.clear()\n",
    "    base_topic_set_copy = base_topic_set[:]\n",
    "    while Count_Topic < 50:\n",
    "        base_topic_index = random.randint(0,len(base_topic_set_copy)-1)\n",
    "        base_topic = base_topic_set_copy[base_topic_index]\n",
    "        indx = int(base_topic[0])\n",
    "        base_topic_indx = int(base_topic[1:3])\n",
    "        #topic_term_dists = np.array(us_topic_variable[indx]['topic_term_dists'])\n",
    "        '''\n",
    "            Calculate JS similarity score\n",
    "        '''\n",
    "        JS_sim = {k : (1- JSD(topic_term_dists[indx][base_topic_indx], topic_term_dists[indx][k])) for k in range(len(topic_term_dists[indx])) if k != base_topic_indx}\n",
    "        JS_topic = max(JS_sim.iteritems(), key=operator.itemgetter(1))[0]\n",
    "        '''\n",
    "            Calculate CS similarity score\n",
    "        '''\n",
    "        CS_sim = {k : cosine_similarity([topic_term_dists[indx][base_topic_indx]], [topic_term_dists[indx][k]])[0][0] for k in range(len(topic_term_dists[indx])) if k != base_topic_indx}\n",
    "        CS_topic = max(CS_sim.iteritems(), key=operator.itemgetter(1))[0]\n",
    "        #print JS_topic,CS_topic\n",
    "        selected_topic = '{0:02d}'.format(JS_topic) + '{0:02d}'.format(CS_topic)\n",
    "        if JS_topic != CS_topic and selected_topic not in candidate_set:\n",
    "            candidate_set.add(selected_topic)\n",
    "            del base_topic_set_copy[base_topic_index]\n",
    "            candidate_list = [\"JS\" + '{0:02d}'.format(JS_topic),\"CS\" + '{0:02d}'.format(CS_topic)]\n",
    "            shuffle(candidate_list)\n",
    "            topic_set = \"sig\" + base_topic + candidate_list[0] + candidate_list[1] + \"\\n\"\n",
    "            print Count_Topic,topic_set\n",
    "            Count_Topic += 1 \n",
    "            f.write(topic_set)\n",
    "        else:\n",
    "            del base_topic_set_copy[base_topic_index]\n",
    "f.close() \n",
    "print \"Finish Preparing JS CS topic pair\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sig830WE48JS18\n",
      "\n",
      "1 sig505JS36WE30\n",
      "\n",
      "2 sig183WE81JS85\n",
      "\n",
      "3 sig775WE44JS12\n",
      "\n",
      "52 52 5252\n",
      "55 55 5555\n",
      "4 sig922WE55JS45\n",
      "\n",
      "5 5 0505\n",
      "5 sig814WE41JS10\n",
      "\n",
      "6 sig402JS50WE66\n",
      "\n",
      "15 15 1515\n",
      "67 67 6767\n",
      "7 sig757WE27JS44\n",
      "\n",
      "8 sig160JS13WE65\n",
      "\n",
      "74 74 7474\n",
      "29 29 2929\n",
      "9 sig749JS10WE06\n",
      "\n",
      "1 1 0101\n",
      "61 61 6161\n",
      "10 sig444JS07WE84\n",
      "\n",
      "22 22 2222\n",
      "11 sig371JS39WE69\n",
      "\n",
      "16 16 1616\n",
      "12 sig211JS52WE10\n",
      "\n",
      "13 sig903JS74WE22\n",
      "\n",
      "14 sig827WE69JS31\n",
      "\n",
      "47 47 4747\n",
      "15 sig360WE58JS27\n",
      "\n",
      "16 sig474WE20JS62\n",
      "\n",
      "7 7 0707\n",
      "27 58 2758\n",
      "17 17 1717\n",
      "17 sig761WE53JS50\n",
      "\n",
      "61 61 6161\n",
      "10 10 1010\n",
      "18 sig138WE34JS87\n",
      "\n",
      "19 sig748WE55JS30\n",
      "\n",
      "20 sig482WE46JS78\n",
      "\n",
      "75 75 7575\n",
      "21 sig756WE59JS06\n",
      "\n",
      "83 83 8383\n",
      "13 13 1313\n",
      "38 38 3838\n",
      "22 sig447WE82JS21\n",
      "\n",
      "23 sig051WE37JS33\n",
      "\n",
      "49 49 4949\n",
      "4 4 0404\n",
      "24 sig271WE87JS52\n",
      "\n",
      "25 sig068JS83WE72\n",
      "\n",
      "26 sig471JS70WE22\n",
      "\n",
      "16 16 1616\n",
      "27 sig977JS03WE22\n",
      "\n",
      "28 sig817WE57JS85\n",
      "\n",
      "88 88 8888\n",
      "29 sig473WE13JS29\n",
      "\n",
      "30 sig609WE06JS46\n",
      "\n",
      "5 5 0505\n",
      "31 sig571WE30JS28\n",
      "\n",
      "32 sig981WE51JS61\n",
      "\n",
      "33 sig105WE52JS78\n",
      "\n",
      "52 87 5287\n",
      "34 sig240WE35JS23\n",
      "\n",
      "35 sig530JS36WE54\n",
      "\n",
      "36 sig635JS43WE00\n",
      "\n",
      "48 48 4848\n",
      "11 11 1111\n",
      "26 26 2626\n",
      "37 sig008JS60WE13\n",
      "\n",
      "2 2 0202\n",
      "38 sig413WE07JS29\n",
      "\n",
      "39 sig780JS17WE76\n",
      "\n",
      "40 sig562WE60JS33\n",
      "\n",
      "41 sig387JS44WE78\n",
      "\n",
      "41 41 4141\n",
      "23 23 2323\n",
      "45 45 4545\n",
      "42 sig011JS22WE33\n",
      "\n",
      "19 19 1919\n",
      "8 8 0808\n",
      "43 sig717WE24JS30\n",
      "\n",
      "44 sig238WE76JS56\n",
      "\n",
      "45 sig978JS03WE44\n",
      "\n",
      "11 11 1111\n",
      "27 27 2727\n",
      "83 83 8383\n",
      "82 82 8282\n",
      "46 sig831WE69JS27\n",
      "\n",
      "47 sig852JS04WE10\n",
      "\n",
      "60 60 6060\n",
      "48 sig920WE41JS69\n",
      "\n",
      "49 sig622JS05WE35\n",
      "\n",
      "Finish Preparing JS WE topic pair\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Preparing the comarison data between JD_divergence and Word_Embedding\n",
    "'''\n",
    "\n",
    "with open(\"sigirtopicset.txt\",\"a\") as f:\n",
    "    '''\n",
    "        Select 50 topic from 300 base topic \n",
    "        could generate different candidiate topic based on selected approach\n",
    "    '''\n",
    "    Count_Topic = 0\n",
    "    candidate_set = set()\n",
    "    candidate_set.clear()\n",
    "    base_topic_set_copy = base_topic_set[:]\n",
    "    while Count_Topic < 50:\n",
    "        base_topic_index = random.randint(0,len(base_topic_set_copy)-1)\n",
    "        base_topic = base_topic_set_copy[base_topic_index]\n",
    "        indx = int(base_topic[0])\n",
    "        base_topic_indx = int(base_topic[1:3])\n",
    "        #topic_term_dists = np.array(us_topic_variable[indx]['topic_term_dists'])\n",
    "        '''\n",
    "            Calculate JS similarity score\n",
    "        '''\n",
    "        JS_sim = {k : (1- JSD(topic_term_dists[indx][base_topic_indx], topic_term_dists[indx][k])) for k in range(len(topic_term_dists[indx])) if k != base_topic_indx}\n",
    "        JS_topic = max(JS_sim.iteritems(), key=operator.itemgetter(1))[0]\n",
    "        '''\n",
    "            Calculate WE similarity score\n",
    "        '''\n",
    "        WE_sim = {k : getTwoTopicsDistance_minipath(topic_top_term_list[indx][base_topic_indx], topic_top_term_list[indx][k],us_word_URL) for k in range(len(topic_term_dists[indx])) if k != base_topic_indx}\n",
    "        #print indx, base_topic_indx, WE_sim\n",
    "        WE_topic = max(WE_sim.iteritems(), key=operator.itemgetter(1))[0]\n",
    "        selected_topic = '{0:02d}'.format(JS_topic) + '{0:02d}'.format(WE_topic)\n",
    "        if JS_topic != WE_topic and selected_topic not in candidate_set:\n",
    "            candidate_set.add(selected_topic)\n",
    "            del base_topic_set_copy[base_topic_index]\n",
    "            candidate_list = [\"JS\" + '{0:02d}'.format(JS_topic),\"WE\" + '{0:02d}'.format(WE_topic)]\n",
    "            shuffle(candidate_list)\n",
    "            topic_set = \"sig\" + base_topic + candidate_list[0] + candidate_list[1] + \"\\n\"\n",
    "            print Count_Topic,topic_set\n",
    "            Count_Topic += 1 \n",
    "            f.write(topic_set)\n",
    "        else:\n",
    "            print JS_topic,WE_topic,selected_topic\n",
    "            del base_topic_set_copy[base_topic_index]\n",
    "f.close() \n",
    "print \"Finish Preparing JS WE topic pair\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sig839HD77CS79\n",
      "\n",
      "1 sig635HD43CS26\n",
      "\n",
      "2 sig845HD40CS79\n",
      "\n",
      "3 sig113CS47HD60\n",
      "\n",
      "4 sig265CS30HD42\n",
      "\n",
      "5 sig517HD43CS27\n",
      "\n",
      "6 sig149CS01HD22\n",
      "\n",
      "7 sig535CS60HD48\n",
      "\n",
      "8 sig421CS47HD37\n",
      "\n",
      "9 sig478CS83HD82\n",
      "\n",
      "10 sig568HD46CS06\n",
      "\n",
      "11 sig962CS88HD22\n",
      "\n",
      "12 sig482CS78HD63\n",
      "\n",
      "13 sig603CS80HD46\n",
      "\n",
      "14 sig089HD76CS48\n",
      "\n",
      "15 sig708HD52CS13\n",
      "\n",
      "16 sig344CS64HD75\n",
      "\n",
      "17 sig160CS47HD13\n",
      "\n",
      "18 sig360CS27HD58\n",
      "\n",
      "19 sig601HD32CS21\n",
      "\n",
      "20 sig734CS72HD54\n",
      "\n",
      "21 sig757HD52CS44\n",
      "\n",
      "22 sig419HD88CS80\n",
      "\n",
      "23 sig643HD35CS17\n",
      "\n",
      "24 sig578CS43HD29\n",
      "\n",
      "25 sig748CS04HD30\n",
      "\n",
      "26 sig775HD12CS00\n",
      "\n",
      "27 sig884HD66CS65\n",
      "\n",
      "28 sig674HD05CS81\n",
      "\n",
      "29 sig327HD65CS77\n",
      "\n",
      "30 sig338CS05HD83\n",
      "\n",
      "31 sig528HD30CS71\n",
      "\n",
      "32 sig304CS00HD36\n",
      "\n",
      "33 sig161HD62CS58\n",
      "\n",
      "34 sig977HD03CS78\n",
      "\n",
      "35 sig142HD56CS17\n",
      "\n",
      "36 sig435HD67CS10\n",
      "\n",
      "37 sig467HD44CS17\n",
      "\n",
      "38 sig354HD16CS67\n",
      "\n",
      "39 sig077HD17CS22\n",
      "\n",
      "40 sig244CS48HD16\n",
      "\n",
      "41 sig560CS04HD62\n",
      "\n",
      "42 sig530HD28CS54\n",
      "\n",
      "43 sig626CS35HD07\n",
      "\n",
      "44 sig061CS57HD69\n",
      "\n",
      "45 sig137HD52CS02\n",
      "\n",
      "46 sig940CS34HD29\n",
      "\n",
      "47 sig814CS10HD85\n",
      "\n",
      "48 sig958CS06HD68\n",
      "\n",
      "49 sig189CS86HD28\n",
      "\n",
      "Finish Preparing HD CS topic pair\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Preparing the comarison data between Hellinger_distance and Cosine Similarity\n",
    "'''\n",
    "\n",
    "with open(\"sigirtopicset.txt\",\"a\") as f:\n",
    "    '''\n",
    "        Select 50 topic from 300 base topic \n",
    "        could generate different candidiate topic based on selected approach\n",
    "    '''\n",
    "    Count_Topic = 0\n",
    "    candidate_set = set()\n",
    "    candidate_set.clear()\n",
    "    base_topic_set_copy = base_topic_set[:]\n",
    "    while Count_Topic < 50:\n",
    "        base_topic_index = random.randint(0,len(base_topic_set_copy)-1)\n",
    "        base_topic = base_topic_set_copy[base_topic_index]\n",
    "        indx = int(base_topic[0])\n",
    "        base_topic_indx = int(base_topic[1:3])\n",
    "        #topic_term_dists = np.array(us_topic_variable[indx]['topic_term_dists'])\n",
    "        '''\n",
    "            Calculate HD similarity score\n",
    "        '''\n",
    "        HD_sim = {k : (1- hellinger(topic_term_dists[indx][base_topic_indx], topic_term_dists[indx][k])) for k in range(len(topic_term_dists[indx])) if k != base_topic_indx} \n",
    "        HD_topic = max(HD_sim.iteritems(), key=operator.itemgetter(1))[0]\n",
    "        '''\n",
    "            Calculate CS similarity score\n",
    "        '''\n",
    "        CS_sim = {k : cosine_similarity([topic_term_dists[indx][base_topic_indx]], [topic_term_dists[indx][k]])[0][0] for k in range(len(topic_term_dists[indx])) if k != base_topic_indx}\n",
    "        CS_topic = max(CS_sim.iteritems(), key=operator.itemgetter(1))[0]\n",
    "        selected_topic = '{0:02d}'.format(HD_topic) + '{0:02d}'.format(CS_topic)\n",
    "        if HD_topic != CS_topic and selected_topic not in candidate_set:\n",
    "            candidate_set.add(selected_topic)\n",
    "            del base_topic_set_copy[base_topic_index]\n",
    "            candidate_list = [\"HD\" + '{0:02d}'.format(HD_topic),\"CS\" + '{0:02d}'.format(CS_topic)]\n",
    "            shuffle(candidate_list)\n",
    "            topic_set = \"sig\" + base_topic + candidate_list[0] + candidate_list[1] + \"\\n\"\n",
    "            print Count_Topic,topic_set\n",
    "            Count_Topic += 1 \n",
    "            f.write(topic_set)\n",
    "        else:\n",
    "            del base_topic_set_copy[base_topic_index]\n",
    "f.close() \n",
    "print \"Finish Preparing HD CS topic pair\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sig444WE84HD07\n",
      "\n",
      "1 sig757WE27HD52\n",
      "\n",
      "2 sig910HD24WE77\n",
      "\n",
      "3 sig825HD27WE81\n",
      "\n",
      "4 sig653WE06HD46\n",
      "\n",
      "5 sig562WE60HD33\n",
      "\n",
      "6 sig447WE82HD21\n",
      "\n",
      "7 sig616HD06WE71\n",
      "\n",
      "8 sig008HD60WE13\n",
      "\n",
      "9 sig048WE89HD76\n",
      "\n",
      "10 sig756WE59HD35\n",
      "\n",
      "11 sig077WE30HD17\n",
      "\n",
      "12 sig272HD34WE19\n",
      "\n",
      "13 sig528WE78HD30\n",
      "\n",
      "14 sig978HD03WE44\n",
      "\n",
      "15 sig571HD28WE30\n",
      "\n",
      "16 sig121HD42WE15\n",
      "\n",
      "17 sig277WE17HD76\n",
      "\n",
      "18 sig944HD48WE52\n",
      "\n",
      "19 sig622HD05WE35\n",
      "\n",
      "20 sig335HD78WE12\n",
      "\n",
      "21 sig903WE22HD74\n",
      "\n",
      "22 sig142WE59HD56\n",
      "\n",
      "23 sig430WE49HD87\n",
      "\n",
      "24 sig482WE46HD63\n",
      "\n",
      "25 sig375HD40WE72\n",
      "\n",
      "26 sig138HD87WE34\n",
      "\n",
      "27 sig022WE17HD11\n",
      "\n",
      "28 sig271HD52WE87\n",
      "\n",
      "29 sig474WE20HD62\n",
      "\n",
      "30 sig120WE51HD04\n",
      "\n",
      "31 sig232HD02WE77\n",
      "\n",
      "32 sig051WE37HD33\n",
      "\n",
      "33 sig977HD03WE22\n",
      "\n",
      "34 sig340WE72HD75\n",
      "\n",
      "35 sig476WE40HD09\n",
      "\n",
      "36 sig804HD21WE52\n",
      "\n",
      "37 sig419HD88WE31\n",
      "\n",
      "38 sig481WE72HD20\n",
      "\n",
      "39 sig981HD61WE51\n",
      "\n",
      "40 sig238WE76HD56\n",
      "\n",
      "41 sig161WE15HD62\n",
      "\n",
      "42 sig158WE21HD61\n",
      "\n",
      "43 sig251HD56WE82\n",
      "\n",
      "44 sig830WE48HD18\n",
      "\n",
      "45 sig010HD76WE48\n",
      "\n",
      "46 sig448HD37WE27\n",
      "\n",
      "47 sig409WE44HD76\n",
      "\n",
      "48 sig920WE41HD69\n",
      "\n",
      "49 sig413HD29WE07\n",
      "\n",
      "Finish Preparing HD WE topic pair\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Preparing the comarison data between Hellinger_distance and Word_Embedding Similarity\n",
    "'''\n",
    "\n",
    "with open(\"sigirtopicset.txt\",\"a\") as f:\n",
    "    '''\n",
    "        Select 50 topic from 300 base topic \n",
    "        could generate different candidiate topic based on selected approach\n",
    "    '''\n",
    "    Count_Topic = 0\n",
    "    candidate_set = set()\n",
    "    candidate_set.clear()\n",
    "    base_topic_set_copy = base_topic_set[:]\n",
    "    while Count_Topic < 50:\n",
    "        base_topic_index = random.randint(0,len(base_topic_set_copy)-1)\n",
    "        base_topic = base_topic_set_copy[base_topic_index]\n",
    "        indx = int(base_topic[0])\n",
    "        base_topic_indx = int(base_topic[1:3])\n",
    "        #topic_term_dists = np.array(us_topic_variable[indx]['topic_term_dists'])\n",
    "        '''\n",
    "            Calculate HD similarity score\n",
    "        '''\n",
    "        HD_sim = {k : (1- hellinger(topic_term_dists[indx][base_topic_indx], topic_term_dists[indx][k])) for k in range(len(topic_term_dists[indx])) if k != base_topic_indx} \n",
    "        HD_topic = max(HD_sim.iteritems(), key=operator.itemgetter(1))[0]\n",
    "        '''\n",
    "            Calculate WE similarity score\n",
    "        '''\n",
    "        WE_sim = {k : getTwoTopicsDistance_minipath(topic_top_term_list[indx][base_topic_indx], topic_top_term_list[indx][k],us_word_URL) for k in range(len(topic_term_dists[indx])) if k != base_topic_indx}\n",
    "        WE_topic = max(WE_sim.iteritems(), key=operator.itemgetter(1))[0]\n",
    "        selected_topic = '{0:02d}'.format(HD_topic) + '{0:02d}'.format(WE_topic)\n",
    "        if HD_topic != WE_topic and selected_topic not in candidate_set:\n",
    "            candidate_set.add(selected_topic)\n",
    "            del base_topic_set_copy[base_topic_index]\n",
    "            candidate_list = [\"HD\" + '{0:02d}'.format(HD_topic),\"WE\" + '{0:02d}'.format(WE_topic)]\n",
    "            shuffle(candidate_list)\n",
    "            topic_set = \"sig\" + base_topic + candidate_list[0] + candidate_list[1] + \"\\n\"\n",
    "            print Count_Topic,topic_set\n",
    "            Count_Topic += 1 \n",
    "            f.write(topic_set)\n",
    "        else:\n",
    "            del base_topic_set_copy[base_topic_index]\n",
    "f.close() \n",
    "print \"Finish Preparing HD WE topic pair\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 38 7134\n",
      "0 sig138CS71WE34\n",
      "\n",
      "4 20 6235\n",
      "1 sig420CS62WE35\n",
      "\n",
      "3 44 6478\n",
      "2 sig344WE78CS64\n",
      "\n",
      "2 33 3737\n",
      "8 31 2769\n",
      "3 sig831WE69CS27\n",
      "\n",
      "9 22 4555\n",
      "4 sig922CS45WE55\n",
      "\n",
      "3 39 7171\n",
      "9 77 7822\n",
      "5 sig977WE22CS78\n",
      "\n",
      "4 9 7644\n",
      "6 sig409WE44CS76\n",
      "\n",
      "3 35 7812\n",
      "7 sig335CS78WE12\n",
      "\n",
      "1 87 6565\n",
      "0 32 5555\n",
      "0 3 8361\n",
      "8 sig003WE61CS83\n",
      "\n",
      "0 57 6161\n",
      "2 67 7000\n",
      "9 sig267WE00CS70\n",
      "\n",
      "6 27 4747\n",
      "4 69 1616\n",
      "8 52 0410\n",
      "10 sig852CS04WE10\n",
      "\n",
      "0 68 7272\n",
      "6 88 4242\n",
      "2 52 1110\n",
      "11 sig252WE10CS11\n",
      "\n",
      "8 23 5339\n",
      "12 sig823WE39CS53\n",
      "\n",
      "2 51 5682\n",
      "13 sig251CS56WE82\n",
      "\n",
      "7 6 3549\n",
      "14 sig706CS35WE49\n",
      "\n",
      "9 58 0606\n",
      "1 65 8787\n",
      "8 72 8686\n",
      "0 48 7689\n",
      "15 sig048CS76WE89\n",
      "\n",
      "9 78 7744\n",
      "16 sig978WE44CS77\n",
      "\n",
      "1 34 3977\n",
      "17 sig134CS39WE77\n",
      "\n",
      "0 11 2233\n",
      "18 sig011WE33CS22\n",
      "\n",
      "7 1 0313\n",
      "19 sig701WE13CS03\n",
      "\n",
      "7 46 5151\n",
      "4 47 2182\n",
      "20 sig447WE82CS21\n",
      "\n",
      "3 4 0036\n",
      "21 sig304CS00WE36\n",
      "\n",
      "0 61 5740\n",
      "22 sig061CS57WE40\n",
      "\n",
      "5 88 2378\n",
      "23 sig588WE78CS23\n",
      "\n",
      "6 22 0535\n",
      "24 sig622WE35CS05\n",
      "\n",
      "3 14 7474\n",
      "2 30 6506\n",
      "25 sig230WE06CS65\n",
      "\n",
      "6 53 4606\n",
      "26 sig653CS46WE06\n",
      "\n",
      "6 47 2727\n",
      "6 80 4603\n",
      "27 sig680CS46WE03\n",
      "\n",
      "0 83 3838\n",
      "5 17 2778\n",
      "28 sig517CS27WE78\n",
      "\n",
      "7 48 0455\n",
      "29 sig748WE55CS04\n",
      "\n",
      "2 19 7554\n",
      "30 sig219CS75WE54\n",
      "\n",
      "9 69 2020\n",
      "5 76 1616\n",
      "0 38 8383\n",
      "4 73 2913\n",
      "31 sig473CS29WE13\n",
      "\n",
      "1 20 0451\n",
      "32 sig120CS04WE51\n",
      "\n",
      "1 5 7852\n",
      "33 sig105CS78WE52\n",
      "\n",
      "3 74 1414\n",
      "8 45 7966\n",
      "34 sig845WE66CS79\n",
      "\n",
      "2 32 0277\n",
      "35 sig232CS02WE77\n",
      "\n",
      "2 65 3042\n",
      "36 sig265CS30WE42\n",
      "\n",
      "9 20 6941\n",
      "37 sig920WE41CS69\n",
      "\n",
      "5 71 2830\n",
      "38 sig571WE30CS28\n",
      "\n",
      "2 55 4949\n",
      "7 17 0424\n",
      "39 sig717WE24CS04\n",
      "\n",
      "9 66 0707\n",
      "0 22 1117\n",
      "40 sig022CS11WE17\n",
      "\n",
      "4 29 1313\n",
      "2 40 2335\n",
      "41 sig240CS23WE35\n",
      "\n",
      "9 71 0000\n",
      "3 71 3969\n",
      "42 sig371CS39WE69\n",
      "\n",
      "8 17 4457\n",
      "43 sig817WE57CS44\n",
      "\n",
      "3 9 6828\n",
      "44 sig309WE28CS68\n",
      "\n",
      "7 56 0659\n",
      "45 sig756WE59CS06\n",
      "\n",
      "0 10 7648\n",
      "46 sig010CS76WE48\n",
      "\n",
      "5 60 0462\n",
      "47 sig560WE62CS04\n",
      "\n",
      "6 81 7432\n",
      "48 sig681WE32CS74\n",
      "\n",
      "9 35 5353\n",
      "3 52 4544\n",
      "49 sig352WE44CS45\n",
      "\n",
      "Finish Preparing CS WE topic pair\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Preparing the comarison data between Cosine_Similarity and Word_Embedding Similarity\n",
    "'''\n",
    "\n",
    "with open(\"sigirtopicsettest.txt\",\"a\") as f:\n",
    "    '''\n",
    "        Select 50 topic from 300 base topic \n",
    "        could generate different candidiate topic based on selected approach\n",
    "    '''\n",
    "    Count_Topic = 0\n",
    "    candidate_set = set()\n",
    "    candidate_set.clear()\n",
    "    base_topic_set_copy = base_topic_set[:]\n",
    "    while Count_Topic < 50:\n",
    "        base_topic_index = random.randint(0,len(base_topic_set_copy)-1)\n",
    "        base_topic = base_topic_set_copy[base_topic_index]\n",
    "        indx = int(base_topic[0])\n",
    "        base_topic_indx = int(base_topic[1:3])\n",
    "        #topic_term_dists = np.array(us_topic_variable[indx]['topic_term_dists'])\n",
    "        '''\n",
    "            Calculate CS similarity score\n",
    "        '''\n",
    "        CS_sim = {k : cosine_similarity([topic_term_dists[indx][base_topic_indx]], [topic_term_dists[indx][k]])[0][0] for k in range(len(topic_term_dists[indx])) if k != base_topic_indx}\n",
    "        CS_topic = max(CS_sim.iteritems(), key=operator.itemgetter(1))[0]\n",
    "        '''\n",
    "            Calculate WE similarity score\n",
    "        '''\n",
    "        WE_sim = {k : getTwoTopicsDistance_minipath(topic_top_term_list[indx][base_topic_indx], topic_top_term_list[indx][k],us_word_URL) for k in range(len(topic_term_dists[indx])) if k != base_topic_indx}\n",
    "        WE_topic = max(WE_sim.iteritems(), key=operator.itemgetter(1))[0]\n",
    "        selected_topic = '{0:02d}'.format(CS_topic) + '{0:02d}'.format(WE_topic)\n",
    "        print indx,base_topic_indx,selected_topic\n",
    "        if CS_topic != WE_topic and selected_topic not in candidate_set:\n",
    "            candidate_set.add(selected_topic)\n",
    "            del base_topic_set_copy[base_topic_index]\n",
    "            candidate_list = [\"CS\" + '{0:02d}'.format(CS_topic),\"WE\" + '{0:02d}'.format(WE_topic)]\n",
    "            shuffle(candidate_list)\n",
    "            topic_set = \"sig\" + base_topic + candidate_list[0] + candidate_list[1] + \"\\n\"\n",
    "            print Count_Topic,topic_set\n",
    "            Count_Topic += 1 \n",
    "            f.write(topic_set)\n",
    "        else:\n",
    "            #print \"find outlet!\"\n",
    "            del base_topic_set_copy[base_topic_index]\n",
    "f.close() \n",
    "print \"Finish Preparing CS WE topic pair\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base:38 candidate_topic_1: 71 candidate_topic_2:34\n",
      "base:20 candidate_topic_1: 62 candidate_topic_2:35\n",
      "base:44 candidate_topic_1: 78 candidate_topic_2:64\n",
      "base:31 candidate_topic_1: 69 candidate_topic_2:27\n",
      "base:22 candidate_topic_1: 45 candidate_topic_2:55\n",
      "base:77 candidate_topic_1: 22 candidate_topic_2:78\n",
      "base:9 candidate_topic_1: 44 candidate_topic_2:76\n",
      "base:35 candidate_topic_1: 78 candidate_topic_2:12\n",
      "base:3 candidate_topic_1: 61 candidate_topic_2:83\n",
      "base:67 candidate_topic_1: 0 candidate_topic_2:70\n",
      "base:52 candidate_topic_1: 4 candidate_topic_2:10\n",
      "base:52 candidate_topic_1: 10 candidate_topic_2:11\n",
      "base:23 candidate_topic_1: 39 candidate_topic_2:53\n",
      "base:51 candidate_topic_1: 56 candidate_topic_2:82\n",
      "base:6 candidate_topic_1: 35 candidate_topic_2:49\n",
      "base:48 candidate_topic_1: 76 candidate_topic_2:89\n",
      "base:78 candidate_topic_1: 44 candidate_topic_2:77\n",
      "base:34 candidate_topic_1: 39 candidate_topic_2:77\n",
      "base:11 candidate_topic_1: 33 candidate_topic_2:22\n",
      "base:1 candidate_topic_1: 13 candidate_topic_2:3\n",
      "base:47 candidate_topic_1: 82 candidate_topic_2:21\n",
      "base:4 candidate_topic_1: 0 candidate_topic_2:36\n",
      "base:61 candidate_topic_1: 57 candidate_topic_2:40\n",
      "base:88 candidate_topic_1: 78 candidate_topic_2:23\n",
      "base:22 candidate_topic_1: 35 candidate_topic_2:5\n",
      "base:30 candidate_topic_1: 6 candidate_topic_2:65\n",
      "base:53 candidate_topic_1: 46 candidate_topic_2:6\n",
      "base:80 candidate_topic_1: 46 candidate_topic_2:3\n",
      "base:17 candidate_topic_1: 27 candidate_topic_2:78\n",
      "base:48 candidate_topic_1: 55 candidate_topic_2:4\n",
      "base:19 candidate_topic_1: 75 candidate_topic_2:54\n",
      "base:73 candidate_topic_1: 29 candidate_topic_2:13\n",
      "base:20 candidate_topic_1: 4 candidate_topic_2:51\n",
      "base:5 candidate_topic_1: 78 candidate_topic_2:52\n",
      "base:45 candidate_topic_1: 66 candidate_topic_2:79\n",
      "base:32 candidate_topic_1: 2 candidate_topic_2:77\n",
      "base:65 candidate_topic_1: 30 candidate_topic_2:42\n",
      "base:20 candidate_topic_1: 41 candidate_topic_2:69\n",
      "base:71 candidate_topic_1: 30 candidate_topic_2:28\n",
      "base:17 candidate_topic_1: 24 candidate_topic_2:4\n",
      "base:22 candidate_topic_1: 11 candidate_topic_2:17\n",
      "base:40 candidate_topic_1: 23 candidate_topic_2:35\n",
      "base:71 candidate_topic_1: 39 candidate_topic_2:69\n",
      "base:17 candidate_topic_1: 57 candidate_topic_2:44\n",
      "base:9 candidate_topic_1: 28 candidate_topic_2:68\n",
      "base:56 candidate_topic_1: 59 candidate_topic_2:6\n",
      "base:10 candidate_topic_1: 76 candidate_topic_2:48\n",
      "base:60 candidate_topic_1: 62 candidate_topic_2:4\n",
      "base:81 candidate_topic_1: 32 candidate_topic_2:74\n",
      "base:52 candidate_topic_1: 44 candidate_topic_2:45\n"
     ]
    }
   ],
   "source": [
    "topic_txt_dict_set = []\n",
    "for idx, var in enumerate(us_topic_variable):\n",
    "    '''\n",
    "        Initialize the words of each topic\n",
    "    '''\n",
    "    topic_txt_dict = {}\n",
    "    with open(us_topic_txt_file[idx]) as text_f:\n",
    "        lines = text_f.readlines()\n",
    "        for line in lines:\n",
    "            values = line.split('\\t')\n",
    "            topic_txt_dict[values[0]] = values[1]\n",
    "    f.close()\n",
    "    topic_txt_dict_set.append(topic_txt_dict.copy())\n",
    "with open(\"sigirtest.txt\",\"w\") as f:\n",
    "    with open(\"sigirtopicsettest.txt\",\"r\") as topic_setf:\n",
    "        lines = topic_setf.readlines()\n",
    "        # read topic selection from file\n",
    "        for line in lines:\n",
    "            indx = int(line[3:4])\n",
    "            base_topic = int(line[4:6])\n",
    "            candidate_topic_1 = int(line[8:10])\n",
    "            candidate_topic_2 = int(line[12:14])\n",
    "            print \"base:\" + str(base_topic) + \" candidate_topic_1: \" + str(candidate_topic_1) + \" candidate_topic_2:\" + str(candidate_topic_2)\n",
    "            # write selected topic words into files\n",
    "            topic_txt_dict = topic_txt_dict_set[indx]\n",
    "            topic_string = line + \"\\t\" + topic_txt_dict[str(base_topic)] + \"\\t\" + topic_txt_dict[str(candidate_topic_1)] + \"\\t\" + topic_txt_dict[str(candidate_topic_2)] + \"\\n\"\n",
    "            f.write(topic_string)\n",
    "    topic_setf.close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create User study topic pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us00037030079\n",
      "\n",
      "us00089010067\n",
      "\n",
      "us00051039052\n",
      "\n",
      "us00054089060\n",
      "\n",
      "us00012060082\n",
      "\n",
      "us00047057031\n",
      "\n",
      "us00073017008\n",
      "\n",
      "us00071079043\n",
      "\n",
      "us00046086018\n",
      "\n",
      "us00060066067\n",
      "\n",
      "us00070054026\n",
      "\n",
      "us00017041035\n",
      "\n",
      "us00009031000\n",
      "\n",
      "us00026053084\n",
      "\n",
      "us00059079035\n",
      "\n",
      "us00003074064\n",
      "\n",
      "us00020080012\n",
      "\n",
      "us00022060054\n",
      "\n",
      "us00015068025\n",
      "\n",
      "us00011025088\n",
      "\n",
      "us00069016015\n",
      "\n",
      "us00042036003\n",
      "\n",
      "us00005037002\n",
      "\n",
      "us00016069017\n",
      "\n",
      "us00050044038\n",
      "\n",
      "us00085058023\n",
      "\n",
      "us00064015075\n",
      "\n",
      "us00004007013\n",
      "\n",
      "us00075026057\n",
      "\n",
      "us00021031069\n",
      "\n",
      "us01037031005\n",
      "\n",
      "us01089060067\n",
      "\n",
      "us01051076039\n",
      "\n",
      "us01054070060\n",
      "\n",
      "us01012054022\n",
      "\n",
      "us01047011072\n",
      "\n",
      "us01073086029\n",
      "\n",
      "us01071083066\n",
      "\n",
      "us01046082086\n",
      "\n",
      "us01060022067\n",
      "\n",
      "us01070026089\n",
      "\n",
      "us01017008041\n",
      "\n",
      "us01009002006\n",
      "\n",
      "us01026074075\n",
      "\n",
      "us01059035044\n",
      "\n",
      "us01003006036\n",
      "\n",
      "us01020072012\n",
      "\n",
      "us01022065010\n",
      "\n",
      "us01015030033\n",
      "\n",
      "us01011047088\n",
      "\n",
      "us01069021019\n",
      "\n",
      "us01042058003\n",
      "\n",
      "us01005062014\n",
      "\n",
      "us01016038054\n",
      "\n",
      "us01050013080\n",
      "\n",
      "us01085082052\n",
      "\n",
      "us01064041058\n",
      "\n",
      "us01004007058\n",
      "\n",
      "us01075064026\n",
      "\n",
      "us01021069035\n",
      "\n",
      "us02037005062\n",
      "\n",
      "us02089006067\n",
      "\n",
      "us02051078075\n",
      "\n",
      "us02054060089\n",
      "\n",
      "us02012020088\n",
      "\n",
      "us02047062072\n",
      "\n",
      "us02073017081\n",
      "\n",
      "us02071079087\n",
      "\n",
      "us02046018045\n",
      "\n",
      "us02060081089\n",
      "\n",
      "us02070026054\n",
      "\n",
      "us02017046041\n",
      "\n",
      "us02009000077\n",
      "\n",
      "us02026079074\n",
      "\n",
      "us02059022052\n",
      "\n",
      "us02003030040\n",
      "\n",
      "us02020082012\n",
      "\n",
      "us02022026060\n",
      "\n",
      "us02015030068\n",
      "\n",
      "us02011072047\n",
      "\n",
      "us02069016017\n",
      "\n",
      "us02042030015\n",
      "\n",
      "us02005037072\n",
      "\n",
      "us02016008054\n",
      "\n",
      "us02050013038\n",
      "\n",
      "us02085079049\n",
      "\n",
      "us02064068058\n",
      "\n",
      "us02004018043\n",
      "\n",
      "us02075064057\n",
      "\n",
      "us02021035041\n",
      "\n",
      "us03006081021\n",
      "\n",
      "us03052078083\n",
      "\n",
      "us03015077063\n",
      "\n",
      "us03083034027\n",
      "\n",
      "us03080054000\n",
      "\n",
      "us03034058027\n",
      "\n",
      "us03068085016\n",
      "\n",
      "us03060083027\n",
      "\n",
      "us03021036068\n",
      "\n",
      "us03019072065\n",
      "\n",
      "us03072009025\n",
      "\n",
      "us03041034078\n",
      "\n",
      "us03027083029\n",
      "\n",
      "us03048085040\n",
      "\n",
      "us03004037085\n",
      "\n",
      "us03013003008\n",
      "\n",
      "us03024049047\n",
      "\n",
      "us03064058029\n",
      "\n",
      "us03076044081\n",
      "\n",
      "us03038013054\n",
      "\n",
      "us03035005047\n",
      "\n",
      "us03025062058\n",
      "\n",
      "us03079067040\n",
      "\n",
      "us03066062029\n",
      "\n",
      "us03085016004\n",
      "\n",
      "us03014029078\n",
      "\n",
      "us03087050062\n",
      "\n",
      "us03078014041\n",
      "\n",
      "us03047045077\n",
      "\n",
      "us03089007008\n",
      "\n",
      "us04048042060\n",
      "\n",
      "us04043033058\n",
      "\n",
      "us04061080047\n",
      "\n",
      "us04021063016\n",
      "\n",
      "us04047029074\n",
      "\n",
      "us04064069026\n",
      "\n",
      "us04071074028\n",
      "\n",
      "us04023000055\n",
      "\n",
      "us04013012050\n",
      "\n",
      "us04080030049\n",
      "\n",
      "us04088045000\n",
      "\n",
      "us04029033041\n",
      "\n",
      "us04079036020\n",
      "\n",
      "us04039037054\n",
      "\n",
      "us04040081002\n",
      "\n",
      "us04049018033\n",
      "\n",
      "us04050061089\n",
      "\n",
      "us04077060056\n",
      "\n",
      "us04007005045\n",
      "\n",
      "us04017059086\n",
      "\n",
      "us04068006078\n",
      "\n",
      "us04000004007\n",
      "\n",
      "us04072032065\n",
      "\n",
      "us04058038073\n",
      "\n",
      "us04008044081\n",
      "\n",
      "us04074029066\n",
      "\n",
      "us04082001006\n",
      "\n",
      "us04041080072\n",
      "\n",
      "us04004054022\n",
      "\n",
      "us04022000015\n",
      "\n",
      "us05063044027\n",
      "\n",
      "us05002063033\n",
      "\n",
      "us05018059014\n",
      "\n",
      "us05089059034\n",
      "\n",
      "us05025050052\n",
      "\n",
      "us05013027085\n",
      "\n",
      "us05061053074\n",
      "\n",
      "us05082006001\n",
      "\n",
      "us05032059066\n",
      "\n",
      "us05087032081\n",
      "\n",
      "us05079049007\n",
      "\n",
      "us05003050058\n",
      "\n",
      "us05043016039\n",
      "\n",
      "us05075065005\n",
      "\n",
      "us05008042076\n",
      "\n",
      "us05031078012\n",
      "\n",
      "us05060077006\n",
      "\n",
      "us05007079065\n",
      "\n",
      "us05045078085\n",
      "\n",
      "us05073034038\n",
      "\n",
      "us05023053082\n",
      "\n",
      "us05059027085\n",
      "\n",
      "us05040071046\n",
      "\n",
      "us05034060089\n",
      "\n",
      "us05064034010\n",
      "\n",
      "us05044017025\n",
      "\n",
      "us05017012044\n",
      "\n",
      "us05033032087\n",
      "\n",
      "us05048085005\n",
      "\n",
      "us05077041080\n",
      "\n",
      "us06007020066\n",
      "\n",
      "us06015013064\n",
      "\n",
      "us06077030041\n",
      "\n",
      "us06048027008\n",
      "\n",
      "us06058068083\n",
      "\n",
      "us06083073003\n",
      "\n",
      "us06009059055\n",
      "\n",
      "us06013085012\n",
      "\n",
      "us06032063037\n",
      "\n",
      "us06053074059\n",
      "\n",
      "us06050053025\n",
      "\n",
      "us06034011060\n",
      "\n",
      "us06019051030\n",
      "\n",
      "us06014058054\n",
      "\n",
      "us06059060050\n",
      "\n",
      "us06018059051\n",
      "\n",
      "us06022088023\n",
      "\n",
      "us06017012026\n",
      "\n",
      "us06044027025\n",
      "\n",
      "us06001069027\n",
      "\n",
      "us06085027001\n",
      "\n",
      "us06029041033\n",
      "\n",
      "us06008005044\n",
      "\n",
      "us06046071078\n",
      "\n",
      "us06056081022\n",
      "\n",
      "us06004054014\n",
      "\n",
      "us06021018077\n",
      "\n",
      "us06086017028\n",
      "\n",
      "us06011076034\n",
      "\n",
      "us06054037078\n",
      "\n",
      "us07080041036\n",
      "\n",
      "us07009055018\n",
      "\n",
      "us07052081018\n",
      "\n",
      "us07084081076\n",
      "\n",
      "us07018065049\n",
      "\n",
      "us07068018058\n",
      "\n",
      "us07000022070\n",
      "\n",
      "us07076008011\n",
      "\n",
      "us07017010064\n",
      "\n",
      "us07016032035\n",
      "\n",
      "us07073005006\n",
      "\n",
      "us07054014004\n",
      "\n",
      "us07062056071\n",
      "\n",
      "us07004073039\n",
      "\n",
      "us07079005049\n",
      "\n",
      "us07030047075\n",
      "\n",
      "us07015034017\n",
      "\n",
      "us07060034052\n",
      "\n",
      "us07088064045\n",
      "\n",
      "us07011076027\n",
      "\n",
      "us07027044065\n",
      "\n",
      "us07028077030\n",
      "\n",
      "us07001009050\n",
      "\n",
      "us07033029002\n",
      "\n",
      "us07007045065\n",
      "\n",
      "us07043039078\n",
      "\n",
      "us07086056035\n",
      "\n",
      "us07048034005\n",
      "\n",
      "us07058083073\n",
      "\n",
      "us07025050052\n",
      "\n",
      "us08010040013\n",
      "\n",
      "us08053029023\n",
      "\n",
      "us08022004000\n",
      "\n",
      "us08054066014\n",
      "\n",
      "us08057026018\n",
      "\n",
      "us08062004071\n",
      "\n",
      "us08039014028\n",
      "\n",
      "us08048042058\n",
      "\n",
      "us08014018082\n",
      "\n",
      "us08084027085\n",
      "\n",
      "us08051006040\n",
      "\n",
      "us08082006008\n",
      "\n",
      "us08007065045\n",
      "\n",
      "us08083064014\n",
      "\n",
      "us08067001015\n",
      "\n",
      "us08031009008\n",
      "\n",
      "us08012052050\n",
      "\n",
      "us08058083081\n",
      "\n",
      "us08008024076\n",
      "\n",
      "us08005081001\n",
      "\n",
      "us08000039007\n",
      "\n",
      "us08003083027\n",
      "\n",
      "us08060009006\n",
      "\n",
      "us08030046061\n",
      "\n",
      "us08023053045\n",
      "\n",
      "us08016009035\n",
      "\n",
      "us08068006078\n",
      "\n",
      "us08069085025\n",
      "\n",
      "us08072061020\n",
      "\n",
      "us08027050089\n",
      "\n",
      "us09032059063\n",
      "\n",
      "us09027044081\n",
      "\n",
      "us09055014023\n",
      "\n",
      "us09034012006\n",
      "\n",
      "us09031082051\n",
      "\n",
      "us09036061008\n",
      "\n",
      "us09066032078\n",
      "\n",
      "us09044027017\n",
      "\n",
      "us09020005027\n",
      "\n",
      "us09074066026\n",
      "\n",
      "us09088064000\n",
      "\n",
      "us09056023085\n",
      "\n",
      "us09022015056\n",
      "\n",
      "us09059018028\n",
      "\n",
      "us09038003024\n",
      "\n",
      "us09017059026\n",
      "\n",
      "us09033025049\n",
      "\n",
      "us09054007004\n",
      "\n",
      "us09019077040\n",
      "\n",
      "us09030047072\n",
      "\n",
      "us09041080030\n",
      "\n",
      "us09026025046\n",
      "\n",
      "us09003058024\n",
      "\n",
      "us09014038035\n",
      "\n",
      "us09047029080\n",
      "\n",
      "us09013027001\n",
      "\n",
      "us09001082069\n",
      "\n",
      "us09084001085\n",
      "\n",
      "us09029049057\n",
      "\n",
      "us09008044003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Generate US Topic Pairs\n",
    "'''\n",
    "with open(\"sigir90topicset_cos.txt\", \"a\") as f:\n",
    "    for idx, var in enumerate(us_topic_variable):\n",
    "        '''\n",
    "            Initialize the words of each topic\n",
    "        '''\n",
    "        topic_txt_dict = {}\n",
    "        with open(us_topic_txt_file[idx]) as text_f:\n",
    "            lines = text_f.readlines()\n",
    "            for line in lines:\n",
    "                values = line.split('\\t')\n",
    "                topic_txt_dict[values[0]] = values[1]\n",
    "\n",
    "        # rank the topic and find the top 30 coherence topics from topic list\n",
    "        coh = {k:topic_coh(topic_txt_dict[k], us_topic_URL) for k in topic_txt_dict}\n",
    "        base_topic = sorted(coh, key=coh.get, reverse=True)[:30]\n",
    "        #print type(base_topic)\n",
    "        for i in range(len(base_topic)):\n",
    "            base_topic[i] = int(base_topic[i])\n",
    "        '''\n",
    "            Generate 15 terms for each topic with highest frequency\n",
    "        '''\n",
    "        vocab = var['vocab']\n",
    "        topic_term_dists = var['topic_term_dists']\n",
    "        topic_top_term_list = []\n",
    "        for i in range(len(topic_term_dists)):\n",
    "            vocab_frequency_dict={vocab[j] : topic_term_dists[i][j] for j in range(len(vocab))}\n",
    "            sorted_term = sorted(vocab_frequency_dict, key=vocab_frequency_dict.get, reverse=True)\n",
    "            topic_top_term_list.append(','.join(sorted_term[:15]))\n",
    "            \n",
    "        '''\n",
    "            Generate the topic term distribution\n",
    "        '''\n",
    "        topic_term_dists = np.array(var['topic_term_dists'])\n",
    "        for topic in base_topic:\n",
    "            #print \"--------------\"\n",
    "            cos_sim = {k : cosine_similarity_new(topic_term_dists[topic], topic_term_dists[k]) for k in range(len(topic_term_dists)) if k != topic}\n",
    "            #cos_sim = {k : getTwoTopicsDistance_minipath(topic_top_term_list[topic], topic_top_term_list[k],us_word_URL) for k in range(len(topic_term_dists)) if k != topic}\n",
    "            candidate_topic = sorted(cos_sim, key=cos_sim.get,reverse=True)[:10]\n",
    "            #print candidate_topic\n",
    "            candidate_topic = random.sample(set(candidate_topic),2)\n",
    "            #print candidate_topic\n",
    "            topic_set = \"us\" + '{0:02d}'.format(idx) + '{0:03d}'.format(topic) + '{0:03d}'.format(candidate_topic[0]) + '{0:03d}'.format(candidate_topic[1]) + \"\\n\"\n",
    "            print topic_set\n",
    "            f.write(topic_set)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base:37 candidate_topic_1: 30 candidate_topic_2:79\n",
      "base:89 candidate_topic_1: 10 candidate_topic_2:67\n",
      "base:51 candidate_topic_1: 39 candidate_topic_2:52\n",
      "base:54 candidate_topic_1: 89 candidate_topic_2:60\n",
      "base:12 candidate_topic_1: 60 candidate_topic_2:82\n",
      "base:47 candidate_topic_1: 57 candidate_topic_2:31\n",
      "base:73 candidate_topic_1: 17 candidate_topic_2:8\n",
      "base:71 candidate_topic_1: 79 candidate_topic_2:43\n",
      "base:46 candidate_topic_1: 86 candidate_topic_2:18\n",
      "base:60 candidate_topic_1: 66 candidate_topic_2:67\n",
      "base:70 candidate_topic_1: 54 candidate_topic_2:26\n",
      "base:17 candidate_topic_1: 41 candidate_topic_2:35\n",
      "base:9 candidate_topic_1: 31 candidate_topic_2:0\n",
      "base:26 candidate_topic_1: 53 candidate_topic_2:84\n",
      "base:59 candidate_topic_1: 79 candidate_topic_2:35\n",
      "base:3 candidate_topic_1: 74 candidate_topic_2:64\n",
      "base:20 candidate_topic_1: 80 candidate_topic_2:12\n",
      "base:22 candidate_topic_1: 60 candidate_topic_2:54\n",
      "base:15 candidate_topic_1: 68 candidate_topic_2:25\n",
      "base:11 candidate_topic_1: 25 candidate_topic_2:88\n",
      "base:69 candidate_topic_1: 16 candidate_topic_2:15\n",
      "base:42 candidate_topic_1: 36 candidate_topic_2:3\n",
      "base:5 candidate_topic_1: 37 candidate_topic_2:2\n",
      "base:16 candidate_topic_1: 69 candidate_topic_2:17\n",
      "base:50 candidate_topic_1: 44 candidate_topic_2:38\n",
      "base:85 candidate_topic_1: 58 candidate_topic_2:23\n",
      "base:64 candidate_topic_1: 15 candidate_topic_2:75\n",
      "base:4 candidate_topic_1: 7 candidate_topic_2:13\n",
      "base:75 candidate_topic_1: 26 candidate_topic_2:57\n",
      "base:21 candidate_topic_1: 31 candidate_topic_2:69\n",
      "base:37 candidate_topic_1: 31 candidate_topic_2:5\n",
      "base:89 candidate_topic_1: 60 candidate_topic_2:67\n",
      "base:51 candidate_topic_1: 76 candidate_topic_2:39\n",
      "base:54 candidate_topic_1: 70 candidate_topic_2:60\n",
      "base:12 candidate_topic_1: 54 candidate_topic_2:22\n",
      "base:47 candidate_topic_1: 11 candidate_topic_2:72\n",
      "base:73 candidate_topic_1: 86 candidate_topic_2:29\n",
      "base:71 candidate_topic_1: 83 candidate_topic_2:66\n",
      "base:46 candidate_topic_1: 82 candidate_topic_2:86\n",
      "base:60 candidate_topic_1: 22 candidate_topic_2:67\n",
      "base:70 candidate_topic_1: 26 candidate_topic_2:89\n",
      "base:17 candidate_topic_1: 8 candidate_topic_2:41\n",
      "base:9 candidate_topic_1: 2 candidate_topic_2:6\n",
      "base:26 candidate_topic_1: 74 candidate_topic_2:75\n",
      "base:59 candidate_topic_1: 35 candidate_topic_2:44\n",
      "base:3 candidate_topic_1: 6 candidate_topic_2:36\n",
      "base:20 candidate_topic_1: 72 candidate_topic_2:12\n",
      "base:22 candidate_topic_1: 65 candidate_topic_2:10\n",
      "base:15 candidate_topic_1: 30 candidate_topic_2:33\n",
      "base:11 candidate_topic_1: 47 candidate_topic_2:88\n",
      "base:69 candidate_topic_1: 21 candidate_topic_2:19\n",
      "base:42 candidate_topic_1: 58 candidate_topic_2:3\n",
      "base:5 candidate_topic_1: 62 candidate_topic_2:14\n",
      "base:16 candidate_topic_1: 38 candidate_topic_2:54\n",
      "base:50 candidate_topic_1: 13 candidate_topic_2:80\n",
      "base:85 candidate_topic_1: 82 candidate_topic_2:52\n",
      "base:64 candidate_topic_1: 41 candidate_topic_2:58\n",
      "base:4 candidate_topic_1: 7 candidate_topic_2:58\n",
      "base:75 candidate_topic_1: 64 candidate_topic_2:26\n",
      "base:21 candidate_topic_1: 69 candidate_topic_2:35\n",
      "base:37 candidate_topic_1: 5 candidate_topic_2:62\n",
      "base:89 candidate_topic_1: 6 candidate_topic_2:67\n",
      "base:51 candidate_topic_1: 78 candidate_topic_2:75\n",
      "base:54 candidate_topic_1: 60 candidate_topic_2:89\n",
      "base:12 candidate_topic_1: 20 candidate_topic_2:88\n",
      "base:47 candidate_topic_1: 62 candidate_topic_2:72\n",
      "base:73 candidate_topic_1: 17 candidate_topic_2:81\n",
      "base:71 candidate_topic_1: 79 candidate_topic_2:87\n",
      "base:46 candidate_topic_1: 18 candidate_topic_2:45\n",
      "base:60 candidate_topic_1: 81 candidate_topic_2:89\n",
      "base:70 candidate_topic_1: 26 candidate_topic_2:54\n",
      "base:17 candidate_topic_1: 46 candidate_topic_2:41\n",
      "base:9 candidate_topic_1: 0 candidate_topic_2:77\n",
      "base:26 candidate_topic_1: 79 candidate_topic_2:74\n",
      "base:59 candidate_topic_1: 22 candidate_topic_2:52\n",
      "base:3 candidate_topic_1: 30 candidate_topic_2:40\n",
      "base:20 candidate_topic_1: 82 candidate_topic_2:12\n",
      "base:22 candidate_topic_1: 26 candidate_topic_2:60\n",
      "base:15 candidate_topic_1: 30 candidate_topic_2:68\n",
      "base:11 candidate_topic_1: 72 candidate_topic_2:47\n",
      "base:69 candidate_topic_1: 16 candidate_topic_2:17\n",
      "base:42 candidate_topic_1: 30 candidate_topic_2:15\n",
      "base:5 candidate_topic_1: 37 candidate_topic_2:72\n",
      "base:16 candidate_topic_1: 8 candidate_topic_2:54\n",
      "base:50 candidate_topic_1: 13 candidate_topic_2:38\n",
      "base:85 candidate_topic_1: 79 candidate_topic_2:49\n",
      "base:64 candidate_topic_1: 68 candidate_topic_2:58\n",
      "base:4 candidate_topic_1: 18 candidate_topic_2:43\n",
      "base:75 candidate_topic_1: 64 candidate_topic_2:57\n",
      "base:21 candidate_topic_1: 35 candidate_topic_2:41\n",
      "base:6 candidate_topic_1: 81 candidate_topic_2:21\n",
      "base:52 candidate_topic_1: 78 candidate_topic_2:83\n",
      "base:15 candidate_topic_1: 77 candidate_topic_2:63\n",
      "base:83 candidate_topic_1: 34 candidate_topic_2:27\n",
      "base:80 candidate_topic_1: 54 candidate_topic_2:0\n",
      "base:34 candidate_topic_1: 58 candidate_topic_2:27\n",
      "base:68 candidate_topic_1: 85 candidate_topic_2:16\n",
      "base:60 candidate_topic_1: 83 candidate_topic_2:27\n",
      "base:21 candidate_topic_1: 36 candidate_topic_2:68\n",
      "base:19 candidate_topic_1: 72 candidate_topic_2:65\n",
      "base:72 candidate_topic_1: 9 candidate_topic_2:25\n",
      "base:41 candidate_topic_1: 34 candidate_topic_2:78\n",
      "base:27 candidate_topic_1: 83 candidate_topic_2:29\n",
      "base:48 candidate_topic_1: 85 candidate_topic_2:40\n",
      "base:4 candidate_topic_1: 37 candidate_topic_2:85\n",
      "base:13 candidate_topic_1: 3 candidate_topic_2:8\n",
      "base:24 candidate_topic_1: 49 candidate_topic_2:47\n",
      "base:64 candidate_topic_1: 58 candidate_topic_2:29\n",
      "base:76 candidate_topic_1: 44 candidate_topic_2:81\n",
      "base:38 candidate_topic_1: 13 candidate_topic_2:54\n",
      "base:35 candidate_topic_1: 5 candidate_topic_2:47\n",
      "base:25 candidate_topic_1: 62 candidate_topic_2:58\n",
      "base:79 candidate_topic_1: 67 candidate_topic_2:40\n",
      "base:66 candidate_topic_1: 62 candidate_topic_2:29\n",
      "base:85 candidate_topic_1: 16 candidate_topic_2:4\n",
      "base:14 candidate_topic_1: 29 candidate_topic_2:78\n",
      "base:87 candidate_topic_1: 50 candidate_topic_2:62\n",
      "base:78 candidate_topic_1: 14 candidate_topic_2:41\n",
      "base:47 candidate_topic_1: 45 candidate_topic_2:77\n",
      "base:89 candidate_topic_1: 7 candidate_topic_2:8\n",
      "base:48 candidate_topic_1: 42 candidate_topic_2:60\n",
      "base:43 candidate_topic_1: 33 candidate_topic_2:58\n",
      "base:61 candidate_topic_1: 80 candidate_topic_2:47\n",
      "base:21 candidate_topic_1: 63 candidate_topic_2:16\n",
      "base:47 candidate_topic_1: 29 candidate_topic_2:74\n",
      "base:64 candidate_topic_1: 69 candidate_topic_2:26\n",
      "base:71 candidate_topic_1: 74 candidate_topic_2:28\n",
      "base:23 candidate_topic_1: 0 candidate_topic_2:55\n",
      "base:13 candidate_topic_1: 12 candidate_topic_2:50\n",
      "base:80 candidate_topic_1: 30 candidate_topic_2:49\n",
      "base:88 candidate_topic_1: 45 candidate_topic_2:0\n",
      "base:29 candidate_topic_1: 33 candidate_topic_2:41\n",
      "base:79 candidate_topic_1: 36 candidate_topic_2:20\n",
      "base:39 candidate_topic_1: 37 candidate_topic_2:54\n",
      "base:40 candidate_topic_1: 81 candidate_topic_2:2\n",
      "base:49 candidate_topic_1: 18 candidate_topic_2:33\n",
      "base:50 candidate_topic_1: 61 candidate_topic_2:89\n",
      "base:77 candidate_topic_1: 60 candidate_topic_2:56\n",
      "base:7 candidate_topic_1: 5 candidate_topic_2:45\n",
      "base:17 candidate_topic_1: 59 candidate_topic_2:86\n",
      "base:68 candidate_topic_1: 6 candidate_topic_2:78\n",
      "base:0 candidate_topic_1: 4 candidate_topic_2:7\n",
      "base:72 candidate_topic_1: 32 candidate_topic_2:65\n",
      "base:58 candidate_topic_1: 38 candidate_topic_2:73\n",
      "base:8 candidate_topic_1: 44 candidate_topic_2:81\n",
      "base:74 candidate_topic_1: 29 candidate_topic_2:66\n",
      "base:82 candidate_topic_1: 1 candidate_topic_2:6\n",
      "base:41 candidate_topic_1: 80 candidate_topic_2:72\n",
      "base:4 candidate_topic_1: 54 candidate_topic_2:22\n",
      "base:22 candidate_topic_1: 0 candidate_topic_2:15\n",
      "base:63 candidate_topic_1: 44 candidate_topic_2:27\n",
      "base:2 candidate_topic_1: 63 candidate_topic_2:33\n",
      "base:18 candidate_topic_1: 59 candidate_topic_2:14\n",
      "base:89 candidate_topic_1: 59 candidate_topic_2:34\n",
      "base:25 candidate_topic_1: 50 candidate_topic_2:52\n",
      "base:13 candidate_topic_1: 27 candidate_topic_2:85\n",
      "base:61 candidate_topic_1: 53 candidate_topic_2:74\n",
      "base:82 candidate_topic_1: 6 candidate_topic_2:1\n",
      "base:32 candidate_topic_1: 59 candidate_topic_2:66\n",
      "base:87 candidate_topic_1: 32 candidate_topic_2:81\n",
      "base:79 candidate_topic_1: 49 candidate_topic_2:7\n",
      "base:3 candidate_topic_1: 50 candidate_topic_2:58\n",
      "base:43 candidate_topic_1: 16 candidate_topic_2:39\n",
      "base:75 candidate_topic_1: 65 candidate_topic_2:5\n",
      "base:8 candidate_topic_1: 42 candidate_topic_2:76\n",
      "base:31 candidate_topic_1: 78 candidate_topic_2:12\n",
      "base:60 candidate_topic_1: 77 candidate_topic_2:6\n",
      "base:7 candidate_topic_1: 79 candidate_topic_2:65\n",
      "base:45 candidate_topic_1: 78 candidate_topic_2:85\n",
      "base:73 candidate_topic_1: 34 candidate_topic_2:38\n",
      "base:23 candidate_topic_1: 53 candidate_topic_2:82\n",
      "base:59 candidate_topic_1: 27 candidate_topic_2:85\n",
      "base:40 candidate_topic_1: 71 candidate_topic_2:46\n",
      "base:34 candidate_topic_1: 60 candidate_topic_2:89\n",
      "base:64 candidate_topic_1: 34 candidate_topic_2:10\n",
      "base:44 candidate_topic_1: 17 candidate_topic_2:25\n",
      "base:17 candidate_topic_1: 12 candidate_topic_2:44\n",
      "base:33 candidate_topic_1: 32 candidate_topic_2:87\n",
      "base:48 candidate_topic_1: 85 candidate_topic_2:5\n",
      "base:77 candidate_topic_1: 41 candidate_topic_2:80\n",
      "base:7 candidate_topic_1: 20 candidate_topic_2:66\n",
      "base:15 candidate_topic_1: 13 candidate_topic_2:64\n",
      "base:77 candidate_topic_1: 30 candidate_topic_2:41\n",
      "base:48 candidate_topic_1: 27 candidate_topic_2:8\n",
      "base:58 candidate_topic_1: 68 candidate_topic_2:83\n",
      "base:83 candidate_topic_1: 73 candidate_topic_2:3\n",
      "base:9 candidate_topic_1: 59 candidate_topic_2:55\n",
      "base:13 candidate_topic_1: 85 candidate_topic_2:12\n",
      "base:32 candidate_topic_1: 63 candidate_topic_2:37\n",
      "base:53 candidate_topic_1: 74 candidate_topic_2:59\n",
      "base:50 candidate_topic_1: 53 candidate_topic_2:25\n",
      "base:34 candidate_topic_1: 11 candidate_topic_2:60\n",
      "base:19 candidate_topic_1: 51 candidate_topic_2:30\n",
      "base:14 candidate_topic_1: 58 candidate_topic_2:54\n",
      "base:59 candidate_topic_1: 60 candidate_topic_2:50\n",
      "base:18 candidate_topic_1: 59 candidate_topic_2:51\n",
      "base:22 candidate_topic_1: 88 candidate_topic_2:23\n",
      "base:17 candidate_topic_1: 12 candidate_topic_2:26\n",
      "base:44 candidate_topic_1: 27 candidate_topic_2:25\n",
      "base:1 candidate_topic_1: 69 candidate_topic_2:27\n",
      "base:85 candidate_topic_1: 27 candidate_topic_2:1\n",
      "base:29 candidate_topic_1: 41 candidate_topic_2:33\n",
      "base:8 candidate_topic_1: 5 candidate_topic_2:44\n",
      "base:46 candidate_topic_1: 71 candidate_topic_2:78\n",
      "base:56 candidate_topic_1: 81 candidate_topic_2:22\n",
      "base:4 candidate_topic_1: 54 candidate_topic_2:14\n",
      "base:21 candidate_topic_1: 18 candidate_topic_2:77\n",
      "base:86 candidate_topic_1: 17 candidate_topic_2:28\n",
      "base:11 candidate_topic_1: 76 candidate_topic_2:34\n",
      "base:54 candidate_topic_1: 37 candidate_topic_2:78\n",
      "base:80 candidate_topic_1: 41 candidate_topic_2:36\n",
      "base:9 candidate_topic_1: 55 candidate_topic_2:18\n",
      "base:52 candidate_topic_1: 81 candidate_topic_2:18\n",
      "base:84 candidate_topic_1: 81 candidate_topic_2:76\n",
      "base:18 candidate_topic_1: 65 candidate_topic_2:49\n",
      "base:68 candidate_topic_1: 18 candidate_topic_2:58\n",
      "base:0 candidate_topic_1: 22 candidate_topic_2:70\n",
      "base:76 candidate_topic_1: 8 candidate_topic_2:11\n",
      "base:17 candidate_topic_1: 10 candidate_topic_2:64\n",
      "base:16 candidate_topic_1: 32 candidate_topic_2:35\n",
      "base:73 candidate_topic_1: 5 candidate_topic_2:6\n",
      "base:54 candidate_topic_1: 14 candidate_topic_2:4\n",
      "base:62 candidate_topic_1: 56 candidate_topic_2:71\n",
      "base:4 candidate_topic_1: 73 candidate_topic_2:39\n",
      "base:79 candidate_topic_1: 5 candidate_topic_2:49\n",
      "base:30 candidate_topic_1: 47 candidate_topic_2:75\n",
      "base:15 candidate_topic_1: 34 candidate_topic_2:17\n",
      "base:60 candidate_topic_1: 34 candidate_topic_2:52\n",
      "base:88 candidate_topic_1: 64 candidate_topic_2:45\n",
      "base:11 candidate_topic_1: 76 candidate_topic_2:27\n",
      "base:27 candidate_topic_1: 44 candidate_topic_2:65\n",
      "base:28 candidate_topic_1: 77 candidate_topic_2:30\n",
      "base:1 candidate_topic_1: 9 candidate_topic_2:50\n",
      "base:33 candidate_topic_1: 29 candidate_topic_2:2\n",
      "base:7 candidate_topic_1: 45 candidate_topic_2:65\n",
      "base:43 candidate_topic_1: 39 candidate_topic_2:78\n",
      "base:86 candidate_topic_1: 56 candidate_topic_2:35\n",
      "base:48 candidate_topic_1: 34 candidate_topic_2:5\n",
      "base:58 candidate_topic_1: 83 candidate_topic_2:73\n",
      "base:25 candidate_topic_1: 50 candidate_topic_2:52\n",
      "base:10 candidate_topic_1: 40 candidate_topic_2:13\n",
      "base:53 candidate_topic_1: 29 candidate_topic_2:23\n",
      "base:22 candidate_topic_1: 4 candidate_topic_2:0\n",
      "base:54 candidate_topic_1: 66 candidate_topic_2:14\n",
      "base:57 candidate_topic_1: 26 candidate_topic_2:18\n",
      "base:62 candidate_topic_1: 4 candidate_topic_2:71\n",
      "base:39 candidate_topic_1: 14 candidate_topic_2:28\n",
      "base:48 candidate_topic_1: 42 candidate_topic_2:58\n",
      "base:14 candidate_topic_1: 18 candidate_topic_2:82\n",
      "base:84 candidate_topic_1: 27 candidate_topic_2:85\n",
      "base:51 candidate_topic_1: 6 candidate_topic_2:40\n",
      "base:82 candidate_topic_1: 6 candidate_topic_2:8\n",
      "base:7 candidate_topic_1: 65 candidate_topic_2:45\n",
      "base:83 candidate_topic_1: 64 candidate_topic_2:14\n",
      "base:67 candidate_topic_1: 1 candidate_topic_2:15\n",
      "base:31 candidate_topic_1: 9 candidate_topic_2:8\n",
      "base:12 candidate_topic_1: 52 candidate_topic_2:50\n",
      "base:58 candidate_topic_1: 83 candidate_topic_2:81\n",
      "base:8 candidate_topic_1: 24 candidate_topic_2:76\n",
      "base:5 candidate_topic_1: 81 candidate_topic_2:1\n",
      "base:0 candidate_topic_1: 39 candidate_topic_2:7\n",
      "base:3 candidate_topic_1: 83 candidate_topic_2:27\n",
      "base:60 candidate_topic_1: 9 candidate_topic_2:6\n",
      "base:30 candidate_topic_1: 46 candidate_topic_2:61\n",
      "base:23 candidate_topic_1: 53 candidate_topic_2:45\n",
      "base:16 candidate_topic_1: 9 candidate_topic_2:35\n",
      "base:68 candidate_topic_1: 6 candidate_topic_2:78\n",
      "base:69 candidate_topic_1: 85 candidate_topic_2:25\n",
      "base:72 candidate_topic_1: 61 candidate_topic_2:20\n",
      "base:27 candidate_topic_1: 50 candidate_topic_2:89\n",
      "base:32 candidate_topic_1: 59 candidate_topic_2:63\n",
      "base:27 candidate_topic_1: 44 candidate_topic_2:81\n",
      "base:55 candidate_topic_1: 14 candidate_topic_2:23\n",
      "base:34 candidate_topic_1: 12 candidate_topic_2:6\n",
      "base:31 candidate_topic_1: 82 candidate_topic_2:51\n",
      "base:36 candidate_topic_1: 61 candidate_topic_2:8\n",
      "base:66 candidate_topic_1: 32 candidate_topic_2:78\n",
      "base:44 candidate_topic_1: 27 candidate_topic_2:17\n",
      "base:20 candidate_topic_1: 5 candidate_topic_2:27\n",
      "base:74 candidate_topic_1: 66 candidate_topic_2:26\n",
      "base:88 candidate_topic_1: 64 candidate_topic_2:0\n",
      "base:56 candidate_topic_1: 23 candidate_topic_2:85\n",
      "base:22 candidate_topic_1: 15 candidate_topic_2:56\n",
      "base:59 candidate_topic_1: 18 candidate_topic_2:28\n",
      "base:38 candidate_topic_1: 3 candidate_topic_2:24\n",
      "base:17 candidate_topic_1: 59 candidate_topic_2:26\n",
      "base:33 candidate_topic_1: 25 candidate_topic_2:49\n",
      "base:54 candidate_topic_1: 7 candidate_topic_2:4\n",
      "base:19 candidate_topic_1: 77 candidate_topic_2:40\n",
      "base:30 candidate_topic_1: 47 candidate_topic_2:72\n",
      "base:41 candidate_topic_1: 80 candidate_topic_2:30\n",
      "base:26 candidate_topic_1: 25 candidate_topic_2:46\n",
      "base:3 candidate_topic_1: 58 candidate_topic_2:24\n",
      "base:14 candidate_topic_1: 38 candidate_topic_2:35\n",
      "base:47 candidate_topic_1: 29 candidate_topic_2:80\n",
      "base:13 candidate_topic_1: 27 candidate_topic_2:1\n",
      "base:1 candidate_topic_1: 82 candidate_topic_2:69\n",
      "base:84 candidate_topic_1: 1 candidate_topic_2:85\n",
      "base:29 candidate_topic_1: 49 candidate_topic_2:57\n",
      "base:8 candidate_topic_1: 44 candidate_topic_2:3\n"
     ]
    }
   ],
   "source": [
    "topic_txt_dict_set = []\n",
    "for idx, var in enumerate(us_topic_variable):\n",
    "    '''\n",
    "        Initialize the words of each topic\n",
    "    '''\n",
    "    topic_txt_dict = {}\n",
    "    with open(us_topic_txt_file[idx]) as text_f:\n",
    "        lines = text_f.readlines()\n",
    "        for line in lines:\n",
    "            values = line.split('\\t')\n",
    "            topic_txt_dict[values[0]] = values[1]\n",
    "    f.close()\n",
    "    topic_txt_dict_set.append(topic_txt_dict.copy())\n",
    "with open(\"sigir90_cos.txt\",\"w\") as f:\n",
    "    with open(\"sigir90topicset_cos.txt\",\"r\") as topic_setf:\n",
    "        lines = topic_setf.readlines()\n",
    "        # read topic selection from file\n",
    "        for line in lines:\n",
    "            indx = int(line[2:4])\n",
    "            base_topic = int(line[4:7])\n",
    "            candidate_topic_1 = int(line[7:10])\n",
    "            candidate_topic_2 = int(line[10:13])\n",
    "            print \"base:\" + str(base_topic) + \" candidate_topic_1: \" + str(candidate_topic_1) + \" candidate_topic_2:\" + str(candidate_topic_2)\n",
    "            # write selected topic words into files\n",
    "            topic_txt_dict = topic_txt_dict_set[indx]\n",
    "            topic_string = line + \"\\t\" + topic_txt_dict[str(base_topic)] + \"\\t\" + topic_txt_dict[str(candidate_topic_1)] + \"\\t\" + topic_txt_dict[str(candidate_topic_2)] + \"\\n\"\n",
    "            f.write(topic_string)\n",
    "    topic_setf.close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
